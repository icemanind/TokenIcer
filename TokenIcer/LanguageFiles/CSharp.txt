// This file was Auto Generated with TokenIcer
using System.Collections.Generic;
using System.Text.RegularExpressions;

namespace TokenIcer
{
	/// <summary>
	/// TokenParser
	/// </summary>
	/// <remarks>
	/// TokenParser is the main parser engine for converting input into lexical tokens.
	/// </remarks>
    public partial class TokenParser
    {
        // This dictionary will store our RegEx rules
        protected readonly Dictionary<Tokens, string> _tokens;
        // This dictionary will store our matches
        protected readonly Dictionary<Tokens, MatchCollection> _regExMatchCollection;
        // This input string will store the string to parse
        private string _inputString;
        // This index is used internally so the parser knows where it left off
        private int _index;

        // This is our token enumeration. It holds every token defined in the grammar
		/// <summary>
		/// Tokens is an enumeration of all possible token values.
		/// </summary>
        public enum Tokens
        {
            Undefined = 0,
\$$$\
        }

        // A public setter for our input string
		/// <summary>
		/// InputString Property
		/// </summary>
		/// <value>
		/// The string value that holds the input string.
		/// </value>
        public virtual string InputString
        {
            set
            {
                _inputString = value;
                PrepareRegex();
            }
        }

        // Our Constructor, which simply initializes values
		/// <summary>
		/// Default Constructor
		/// </summary>
		/// <remarks>
		/// The constructor initalizes memory and adds all of the tokens to the token dictionary.
		/// </remarks>
        public TokenParser()
        {
            _tokens = new Dictionary<Tokens, string>();
            _regExMatchCollection = new Dictionary<Tokens, MatchCollection>();
            _index = 0;
            _inputString = string.Empty;

            // These lines add each grammar rule to the dictionary
\$$$\
        }

		// This function preloads the matches based on our rules and the input string
		/// <summary>
		/// PrepareRegex prepares the regex for parsing by pre-matching the Regex tokens.
		/// </summary>
        protected virtual void PrepareRegex()
        {
            _regExMatchCollection.Clear();
            foreach (KeyValuePair<Tokens, string> pair in _tokens)
            {
                _regExMatchCollection.Add(pair.Key, Regex.Matches(_inputString, pair.Value));
            }
        }

        // ResetParser() will reset the parser.
        // Keep in mind that you must set the input string again
		/// <summary>
		/// ResetParser resets the parser to its inital state. Reloading InputString is required.
		/// </summary>
		/// <seealso cref="InputString">
        public virtual void ResetParser()
        {
            _index = 0;
            _inputString = string.Empty;
			_regExMatchCollection.Clear();
        }

        // GetToken() retrieves the next token and returns a token object
		/// <summary>
		/// GetToken gets the next token in queue
		/// </summary>
		/// <remarks>
		/// GetToken attempts to the match the next character(s) using the
		/// Regex rules defined in the dictionary. If a match can not be
		/// located, then an Undefined token will be created with an empty
		/// string value. In addition, the token pointer will be incremented
		/// by one so that this token doesn't attempt to get identified again by
		/// GetToken()
		/// </remarks>
        public virtual Token GetToken()
        {
			// If we are at the end of our input string then
			// we return null to signify the end of our input string.
			// While parsing tokens, you will undoubtedly be in a loop.
			// Having your loop check for a null token is a good way to end that
			// loop
            if (_index >= _inputString.Length)
                return null;

			// Iterate through our prepared matches/Tokens dictionary
            foreach (KeyValuePair<Tokens, MatchCollection> pair in _regExMatchCollection)
            {
			    // Iterate through each prepared match
                foreach (Match match in pair.Value )
                {
				    // If we find a match, update our index pointer and return a new Token object
                    if (match.Index == _index)
                    {
                        _index += match.Length;
                        return new Token(pair.Key, match.Value);
                    }
					 
					if (match.Index > _index)
                    {
                        break;
                    }
                }
            }
			// If execution got here, then we increment our index pointer
			// and return an Undefined token. 
            _index++;
            return new Token(Tokens.Undefined, (_inputString[_index - 1]).ToString());
        }

        // Peek() will retrieve a PeekToken object and will allow you to see the next token
        // that GetToken() will retrieve.
		/// <summary>
		/// Returns the next token that GetToken() will return.
		/// </summary>
		/// <seealso cref="Peek(PeekToken)">
        public virtual PeekToken Peek()
        {
            return Peek(new PeekToken(_index, new Token(Tokens.Undefined, string.Empty)));
        }

        // This is an overload for Peek(). By passing in the last PeekToken object
        // received from Peek(), you can peek ahead to the next token, and the token after that, etc...
		/// <summary>
		/// Returns the next token after the Token passed here
		/// </summary>
		/// <param name="peekToken">The PeekToken token returned from a previous Peek() call</param>
		/// <seealso cref="Peek()">
        public virtual PeekToken Peek(PeekToken peekToken)
        {
            int oldIndex = _index;

            _index = peekToken.TokenIndex;

            if (_index >= _inputString.Length)
            {
                _index = oldIndex;
                return null;
            }

            foreach (KeyValuePair<Tokens, string> pair in _tokens)
            {
                Regex r = new Regex(pair.Value);
                Match m = r.Match(_inputString, _index);

                if (m.Success && m.Index == _index)
                {
                    _index += m.Length;
                    PeekToken pt = new PeekToken(_index, new Token(pair.Key, m.Value));
                    _index = oldIndex;
                    return pt;
                }
            }
            PeekToken pt2 = new PeekToken(_index + 1, new Token(Tokens.Undefined, (_inputString[_index]).ToString()));
            _index = oldIndex;
            return pt2;
        }
    }

    // This defines the PeekToken object
	/// <summary>
	/// A PeekToken object class
	/// </summary>
	/// <remarks>
	/// A PeekToken is a special pointer object that can be used to Peek() several
	/// tokens ahead in the GetToken() queue.
	/// </remarks>
    public partial class PeekToken
    {
        public int TokenIndex { get; set; }

        public Token TokenPeek { get; set; }

        public PeekToken(int index, Token value)
        {
            TokenIndex = index;
            TokenPeek = value;
        }
    }

    // This defines the Token object
	/// <summary>
	/// a Token object class
	/// </summary>
	/// <remarks>
	/// A Token object holds the token and token value.
	/// </remarks>
    public partial class Token
    {
        public TokenParser.Tokens TokenName { get; set; }

        public string TokenValue { get; set; }

        public Token(TokenParser.Tokens name, string value)
        {
            TokenName = name;
            TokenValue = value;
        }
    }
}
